{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xianghao/mambaforge/envs/audiozen_latest_accelerate/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from accelerate import init_empty_weights\n",
    "from accelerate.utils import BnbQuantizationConfig\n",
    "from accelerate.utils import load_and_quantize_model\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops import rearrange\n",
    "from torch.nn import functional\n",
    "\n",
    "EPSILON = np.finfo(np.float32).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        output_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        bidirectional,\n",
    "        sequence_model=\"GRU\",\n",
    "        output_activate_function=\"Tanh\",\n",
    "        num_groups=4,\n",
    "        mogrify_steps=5,\n",
    "        dropout=0.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if sequence_model == \"LSTM\":\n",
    "            self.sequence_model = nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        elif sequence_model == \"GRU\":\n",
    "            self.sequence_model = nn.GRU(\n",
    "                input_size=input_size,\n",
    "                hidden_size=hidden_size,\n",
    "                num_layers=num_layers,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=dropout,\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Not implemented {sequence_model}\")\n",
    "\n",
    "        # Fully connected layer\n",
    "        if int(output_size):\n",
    "            if bidirectional:\n",
    "                self.fc_output_layer = nn.Linear(hidden_size * 2, output_size)\n",
    "            else:\n",
    "                self.fc_output_layer = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        # Activation function layer\n",
    "        if output_activate_function:\n",
    "            if output_activate_function == \"Tanh\":\n",
    "                self.activate_function = nn.Tanh()\n",
    "            elif output_activate_function == \"ReLU\":\n",
    "                self.activate_function = nn.ReLU()\n",
    "            elif output_activate_function == \"ReLU6\":\n",
    "                self.activate_function = nn.ReLU6()\n",
    "            elif output_activate_function == \"LeakyReLU\":\n",
    "                self.activate_function = nn.LeakyReLU()\n",
    "            elif output_activate_function == \"PReLU\":\n",
    "                self.activate_function = nn.PReLU()\n",
    "            else:\n",
    "                raise NotImplementedError(\n",
    "                    f\"Not implemented activation function {self.activate_function}\"\n",
    "                )\n",
    "\n",
    "        self.output_activate_function = output_activate_function\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # only for custom_lstm and are needed to clean up\n",
    "        self.sequence_model_name = sequence_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: [B, F, T]\n",
    "        Returns:\n",
    "            [B, F, T]\n",
    "        \"\"\"\n",
    "        assert x.dim() == 3, f\"Shape is {x.shape}.\"\n",
    "        batch_size, _, _ = x.shape\n",
    "\n",
    "        if self.sequence_model_name == \"LayerNormLSTM\":\n",
    "            states = [\n",
    "                (\n",
    "                    torch.zeros(batch_size, self.hidden_size, device=x.device),\n",
    "                    torch.zeros(batch_size, self.hidden_size, device=x.device),\n",
    "                )\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "        else:\n",
    "            states = None\n",
    "\n",
    "        x = x.permute(2, 0, 1).contiguous()  # [B, F, T] => [T, B, F]\n",
    "        o, _ = self.sequence_model(x, states)  # [T, B, F] => [T, B, F]\n",
    "\n",
    "        if self.output_size:\n",
    "            o = self.fc_output_layer(o)  # [T, B, F] => [T, B, F]\n",
    "\n",
    "        if self.output_activate_function:\n",
    "            o = self.activate_function(o)\n",
    "\n",
    "        return o.permute(1, 2, 0).contiguous()  # [T, B, F] => [B, F, T]\n",
    "\n",
    "\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseModel, self).__init__()\n",
    "\n",
    "    @staticmethod\n",
    "    def offline_laplace_norm(input, return_mu=False):\n",
    "        \"\"\"Normalize the input with the utterance-level mean.\n",
    "\n",
    "        Args:\n",
    "            input: [B, C, F, T]\n",
    "\n",
    "        Returns:\n",
    "            [B, C, F, T]\n",
    "\n",
    "        Notes:\n",
    "            As mentioned in the paper, the offline normalization is used.\n",
    "            Based on a bunch of experiments, the offline normalization have the same performance as the cumulative one and have a faster convergence than the cumulative one.\n",
    "            Therefore, we use the offline normalization as the default normalization method.\n",
    "        \"\"\"\n",
    "        # utterance-level mu\n",
    "        mu = torch.mean(input, dim=list(range(1, input.dim())), keepdim=True)\n",
    "\n",
    "        normed = input / (mu + EPSILON)\n",
    "\n",
    "        if return_mu:\n",
    "            return normed, mu\n",
    "        else:\n",
    "            return normed\n",
    "\n",
    "    @staticmethod\n",
    "    def cumulative_laplace_norm(input):\n",
    "        \"\"\"Normalize the input with the cumulative mean\n",
    "\n",
    "        Args:\n",
    "            input: [B, C, F, T]\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        batch_size, num_channels, num_freqs, num_frames = input.size()\n",
    "        input = input.reshape(batch_size * num_channels, num_freqs, num_frames)\n",
    "\n",
    "        step_sum = torch.sum(input, dim=1)  # [B * C, F, T] => [B, T]\n",
    "        cumulative_sum = torch.cumsum(step_sum, dim=-1)  # [B, T]\n",
    "\n",
    "        entry_count = torch.arange(\n",
    "            num_freqs,\n",
    "            num_freqs * num_frames + 1,\n",
    "            num_freqs,\n",
    "            dtype=input.dtype,\n",
    "            device=input.device,\n",
    "        )\n",
    "        entry_count = entry_count.reshape(1, num_frames)  # [1, T]\n",
    "        entry_count = entry_count.expand_as(cumulative_sum)  # [1, T] => [B, T]\n",
    "\n",
    "        cumulative_mean = cumulative_sum / entry_count  # B, T\n",
    "        cumulative_mean = cumulative_mean.reshape(\n",
    "            batch_size * num_channels, 1, num_frames\n",
    "        )\n",
    "\n",
    "        normed = input / (cumulative_mean + EPSILON)\n",
    "\n",
    "        return normed.reshape(batch_size, num_channels, num_freqs, num_frames)\n",
    "\n",
    "    @staticmethod\n",
    "    def offline_gaussian_norm(input):\n",
    "        \"\"\"\n",
    "        Zero-Norm\n",
    "        Args:\n",
    "            input: [B, C, F, T]\n",
    "\n",
    "        Returns:\n",
    "            [B, C, F, T]\n",
    "        \"\"\"\n",
    "        mu = torch.mean(input, dim=list(range(1, input.dim())), keepdim=True)\n",
    "        std = torch.std(input, dim=list(range(1, input.dim())), keepdim=True)\n",
    "\n",
    "        normed = (input - mu) / (std + EPSILON)\n",
    "        return normed\n",
    "\n",
    "    def norm_wrapper(self, norm_type: str):\n",
    "        if norm_type == \"offline_laplace_norm\":\n",
    "            norm = self.offline_laplace_norm\n",
    "        elif norm_type == \"cumulative_laplace_norm\":\n",
    "            norm = self.cumulative_laplace_norm\n",
    "        elif norm_type == \"offline_gaussian_norm\":\n",
    "            norm = self.offline_gaussian_norm\n",
    "        else:\n",
    "            raise NotImplementedError(\n",
    "                \"You must set up a type of Norm. \"\n",
    "                \"e.g. offline_laplace_norm, cumulative_laplace_norm, forgetting_norm, etc.\"\n",
    "            )\n",
    "        return norm\n",
    "\n",
    "\n",
    "class SubBandSequenceWrapper(SequenceModel):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def forward(self, subband_input):\n",
    "        (\n",
    "            batch_size,\n",
    "            num_subband_units,\n",
    "            num_channels,\n",
    "            num_subband_freqs,\n",
    "            num_frames,\n",
    "        ) = subband_input.shape\n",
    "        assert num_channels == 1\n",
    "\n",
    "        output = subband_input.reshape(\n",
    "            batch_size * num_subband_units, num_subband_freqs, num_frames\n",
    "        )\n",
    "        output = super().forward(output)\n",
    "\n",
    "        # [B, N, C, 2, center, T]\n",
    "        output = output.reshape(batch_size, num_subband_units, 2, -1, num_frames)\n",
    "\n",
    "        # [B, 2, N, center, T]\n",
    "        output = output.permute(0, 2, 1, 3, 4).contiguous()\n",
    "\n",
    "        # [B, C, N * F_subband_out, T]\n",
    "        output = output.reshape(batch_size, 2, -1, num_frames)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class SubbandModel(BaseModel):\n",
    "    def __init__(\n",
    "        self,\n",
    "        freq_cutoffs,\n",
    "        sb_num_center_freqs,\n",
    "        sb_num_neighbor_freqs,\n",
    "        fb_num_center_freqs,\n",
    "        fb_num_neighbor_freqs,\n",
    "        sequence_model,\n",
    "        hidden_size,\n",
    "        activate_function=False,\n",
    "        norm_type=\"offline_laplace_norm\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        sb_models = []\n",
    "        for (\n",
    "            sb_num_center_freq,\n",
    "            sb_num_neighbor_freq,\n",
    "            fb_num_center_freq,\n",
    "            fb_num_neighbor_freq,\n",
    "        ) in zip(\n",
    "            sb_num_center_freqs,\n",
    "            sb_num_neighbor_freqs,\n",
    "            fb_num_center_freqs,\n",
    "            fb_num_neighbor_freqs,\n",
    "        ):\n",
    "            sb_models.append(\n",
    "                SubBandSequenceWrapper(\n",
    "                    input_size=(sb_num_center_freq + sb_num_neighbor_freq * 2)\n",
    "                    + (fb_num_center_freq + fb_num_neighbor_freq * 2),\n",
    "                    output_size=sb_num_center_freq * 2,\n",
    "                    hidden_size=hidden_size,\n",
    "                    num_layers=2,\n",
    "                    sequence_model=sequence_model,\n",
    "                    bidirectional=False,\n",
    "                    output_activate_function=activate_function,\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.sb_models = nn.ModuleList(sb_models)\n",
    "        self.freq_cutoffs = freq_cutoffs\n",
    "        self.sb_num_center_freqs = sb_num_center_freqs\n",
    "        self.sb_num_neighbor_freqs = sb_num_neighbor_freqs\n",
    "        self.fb_num_center_freqs = fb_num_center_freqs\n",
    "        self.fb_num_neighbor_freqs = fb_num_neighbor_freqs\n",
    "\n",
    "        self.norm = self.norm_wrapper(norm_type)\n",
    "\n",
    "    def _freq_unfold(\n",
    "        self,\n",
    "        input: torch.Tensor,\n",
    "        lower_cutoff_freq=0,\n",
    "        upper_cutoff_freq=20,\n",
    "        num_center_freqs=1,\n",
    "        num_neighbor_freqs=15,\n",
    "    ):\n",
    "        \"\"\"Unfold frequency axis.\n",
    "\n",
    "        Args:\n",
    "            input: magnitude spectrogram of shape (batch_size, 1, num_freqs, num_frames).\n",
    "            cutoff_freq_lower: lower cutoff frequency.\n",
    "            cutoff_freq_higher: higher cutoff frequency.\n",
    "            num_center_freqs: number of center frequencies.\n",
    "            num_neighbor_freqs: number of neighbor frequencies.\n",
    "\n",
    "        Returns:\n",
    "            [batch_size, num_subband_units, num_channels, num_subband_freqs, num_frames]\n",
    "\n",
    "        Note:\n",
    "            We assume that the num_neighbor_freqs should less than the minimum subband intervel.\n",
    "        \"\"\"\n",
    "        batch_size, num_channels, num_freqs, num_frames = input.shape\n",
    "        assert num_channels == 1, \"Only mono audio is supported.\"\n",
    "\n",
    "        if (upper_cutoff_freq - lower_cutoff_freq) % num_center_freqs != 0:\n",
    "            raise ValueError(\n",
    "                f\"The number of center frequencies should be divisible by the subband freqency interval. \"\n",
    "                f\"Got {num_center_freqs=}, {upper_cutoff_freq=}, and {lower_cutoff_freq=}. \"\n",
    "                f\"The subband freqency interval is {upper_cutoff_freq-lower_cutoff_freq}.\"\n",
    "            )\n",
    "\n",
    "        # extract valid input with the shape of [batch_size, 1, num_freqs, num_frames]\n",
    "        if lower_cutoff_freq == 0:\n",
    "            # lower = 0, upper = upper_cutoff_freq + num_neighbor_freqs\n",
    "            valid_input = input[..., 0 : (upper_cutoff_freq + num_neighbor_freqs), :]\n",
    "            valid_input = functional.pad(\n",
    "                input=valid_input,\n",
    "                pad=(0, 0, num_neighbor_freqs, 0),\n",
    "                mode=\"reflect\",\n",
    "            )\n",
    "\n",
    "        elif upper_cutoff_freq == num_freqs:\n",
    "            # lower = lower_cutoff_freq - num_neighbor_freqs, upper = num_freqs\n",
    "            valid_input = input[\n",
    "                ..., lower_cutoff_freq - num_neighbor_freqs : num_freqs, :\n",
    "            ]\n",
    "\n",
    "            valid_input = functional.pad(\n",
    "                input=valid_input,\n",
    "                pad=(0, 0, 0, num_neighbor_freqs),\n",
    "                mode=\"reflect\",\n",
    "            )\n",
    "        else:\n",
    "            # lower = lower_cutoff_freq - num_neighbor_freqs, upper = upper_cutoff_freq + num_neighbor_freqs\n",
    "            valid_input = input[\n",
    "                ...,\n",
    "                lower_cutoff_freq\n",
    "                - num_neighbor_freqs : upper_cutoff_freq\n",
    "                + num_neighbor_freqs,\n",
    "                :,\n",
    "            ]\n",
    "\n",
    "        # unfold\n",
    "        # [B, C * kernel_size, N]\n",
    "        subband_unit_width = num_center_freqs + num_neighbor_freqs * 2\n",
    "        output = functional.unfold(\n",
    "            input=valid_input,\n",
    "            kernel_size=(subband_unit_width, num_frames),\n",
    "            stride=(num_center_freqs, num_frames),\n",
    "        )\n",
    "        num_subband_units = output.shape[-1]\n",
    "\n",
    "        output = output.reshape(\n",
    "            batch_size,\n",
    "            num_channels,\n",
    "            subband_unit_width,\n",
    "            num_frames,\n",
    "            num_subband_units,\n",
    "        )\n",
    "\n",
    "        # [B, N, C, F_subband, T]\n",
    "        output = output.permute(0, 4, 1, 2, 3).contiguous()\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, noisy_input, fb_output):\n",
    "        \"\"\"Forward pass.\n",
    "\n",
    "        Args:\n",
    "            input: magnitude spectrogram of shape (batch_size, 1, num_freqs, num_frames).\n",
    "        \"\"\"\n",
    "        batch_size, num_channels, num_freqs, num_frames = noisy_input.size()\n",
    "        assert num_channels == 1, \"Only mono audio is supported.\"\n",
    "\n",
    "        subband_output = []\n",
    "        for sb_idx, sb_model in enumerate(self.sb_models):\n",
    "            if sb_idx == 0:\n",
    "                lower_cutoff_freq = 0\n",
    "                upper_cutoff_freq = self.freq_cutoffs[0]\n",
    "            elif sb_idx == len(self.sb_models) - 1:\n",
    "                lower_cutoff_freq = self.freq_cutoffs[-1]\n",
    "                upper_cutoff_freq = num_freqs\n",
    "            else:\n",
    "                lower_cutoff_freq = self.freq_cutoffs[sb_idx - 1]\n",
    "                upper_cutoff_freq = self.freq_cutoffs[sb_idx]\n",
    "\n",
    "            # unfold frequency axis\n",
    "            # [B, N, C, F_subband, T]\n",
    "            noisy_subband = self._freq_unfold(\n",
    "                noisy_input,\n",
    "                lower_cutoff_freq,\n",
    "                upper_cutoff_freq,\n",
    "                self.sb_num_center_freqs[sb_idx],\n",
    "                self.sb_num_neighbor_freqs[sb_idx],\n",
    "            )\n",
    "\n",
    "            # [B, N, C, F_subband, T]\n",
    "            fb_subband = self._freq_unfold(\n",
    "                fb_output,\n",
    "                lower_cutoff_freq,\n",
    "                upper_cutoff_freq,\n",
    "                self.fb_num_center_freqs[sb_idx],\n",
    "                self.fb_num_neighbor_freqs[sb_idx],\n",
    "            )\n",
    "\n",
    "            sb_model_input = torch.cat([noisy_subband, fb_subband], dim=-2)\n",
    "            sb_model_input = self.norm(sb_model_input)\n",
    "            subband_output.append(sb_model(sb_model_input))\n",
    "\n",
    "        # [B, C, F, T]\n",
    "        output = torch.cat(subband_output, dim=-2)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class Separator(BaseModel):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Requires:\n",
    "        einops: install with `pip install einops`\n",
    "\n",
    "    Args:\n",
    "        nn: _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sr,\n",
    "        n_fft,\n",
    "        hop_length,\n",
    "        win_length,\n",
    "        fdrc,\n",
    "        num_freqs,\n",
    "        freq_cutoffs,\n",
    "        sb_num_center_freqs,\n",
    "        sb_num_neighbor_freqs,\n",
    "        fb_num_center_freqs,\n",
    "        fb_num_neighbor_freqs,\n",
    "        fb_hidden_size,\n",
    "        sb_hidden_size,\n",
    "        sequence_model,\n",
    "        fb_output_activate_function,\n",
    "        sb_output_activate_function,\n",
    "        norm_type,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.fdrc = fdrc\n",
    "\n",
    "        self.fb_model = SequenceModel(\n",
    "            input_size=num_freqs,\n",
    "            output_size=num_freqs,\n",
    "            hidden_size=fb_hidden_size,\n",
    "            num_layers=2,\n",
    "            bidirectional=False,\n",
    "            sequence_model=sequence_model,\n",
    "            output_activate_function=fb_output_activate_function,\n",
    "        )\n",
    "\n",
    "        self.sb_model = SubbandModel(\n",
    "            freq_cutoffs=freq_cutoffs,\n",
    "            sb_num_center_freqs=sb_num_center_freqs,\n",
    "            sb_num_neighbor_freqs=sb_num_neighbor_freqs,\n",
    "            fb_num_center_freqs=fb_num_center_freqs,\n",
    "            fb_num_neighbor_freqs=fb_num_neighbor_freqs,\n",
    "            hidden_size=sb_hidden_size,\n",
    "            sequence_model=sequence_model,\n",
    "            activate_function=sb_output_activate_function,\n",
    "        )\n",
    "\n",
    "        self.norm = self.norm_wrapper(norm_type)\n",
    "\n",
    "    def forward(self, y):\n",
    "        ndim = y.dim()\n",
    "        assert ndim in (2, 3), \"Input must be 2D (B, T) or 3D tensor (B, 1, T)\"\n",
    "\n",
    "        if ndim == 3:\n",
    "            assert y.size(1) == 1, \"Input must be 2D (B, T) or 3D tensor (B, 1, T)\"\n",
    "            y = y.squeeze(1)\n",
    "\n",
    "        complex_stft = torch.stft(\n",
    "            y,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            window=torch.hann_window(self.win_length, device=y.device),\n",
    "            return_complex=True,\n",
    "        )  # [B, F, T]\n",
    "        complex_stft_view_real = torch.view_as_real(complex_stft)  # [B, F, T, 2]\n",
    "\n",
    "        noisy_mag = torch.abs(complex_stft.unsqueeze(1))  # [B, 1, F, T]\n",
    "\n",
    "        # ================== Fullband ==================\n",
    "        noisy_mag = noisy_mag**self.fdrc  # fdrc\n",
    "        noisy_mag = noisy_mag[..., :-1, :]  # [B, 1, F, T]\n",
    "        fb_input = rearrange(self.norm(noisy_mag), \"b c f t -> b (c f) t\")\n",
    "        fb_output = self.fb_model(fb_input)  # [B, F, T]\n",
    "        fb_output = rearrange(fb_output, \"b f t -> b 1 f t\")\n",
    "\n",
    "        # ================== Subband ==================\n",
    "        cRM = self.sb_model(noisy_mag, fb_output)  # [B, 2, F, T]\n",
    "        cRM = functional.pad(cRM, (0, 0, 0, 1), mode=\"constant\", value=0.0)\n",
    "\n",
    "        # ================== Masking ==================\n",
    "        complex_stft_view_real = rearrange(complex_stft_view_real, \"b f t c -> b c f t\")\n",
    "        enhanced_spec = cRM * complex_stft_view_real  # [B, 2, F, T]\n",
    "\n",
    "        # ================== ISTFT ==================\n",
    "        enhanced_complex = torch.complex(\n",
    "            enhanced_spec[:, 0, ...], enhanced_spec[:, 1, ...]\n",
    "        )\n",
    "\n",
    "        # Magnitude\n",
    "        enhanced_mag = torch.abs(enhanced_complex)  # [B, F, T]\n",
    "\n",
    "        enhanced_y = torch.istft(\n",
    "            enhanced_complex,\n",
    "            n_fft=self.n_fft,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            window=torch.hann_window(self.win_length, device=y.device),\n",
    "            length=y.size(-1),\n",
    "        )\n",
    "        enhanced_y = enhanced_y  # [B, 1, T]\n",
    "        return enhanced_y, enhanced_mag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with init_empty_weights():\n",
    "    model = Separator(\n",
    "        sr = 16000,\n",
    "        fdrc = 0.5,\n",
    "        n_fft = 512,\n",
    "        hop_length = 128,\n",
    "        win_length = 512,\n",
    "        num_freqs = 256,\n",
    "        sequence_model = \"LSTM\",\n",
    "        fb_hidden_size = 512,\n",
    "        fb_output_activate_function = False,\n",
    "        freq_cutoffs = [20, 80],\n",
    "        sb_num_center_freqs = [1, 4, 8],\n",
    "        sb_num_neighbor_freqs = [15, 15, 15],\n",
    "        fb_num_center_freqs = [1, 4, 8],\n",
    "        fb_num_neighbor_freqs = [0, 0, 0],\n",
    "        sb_hidden_size = 384,\n",
    "        sb_output_activate_function = False,\n",
    "        norm_type = \"offline_laplace_norm\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_weights_path =  \"/home/xianghao/proj/audiozen/recipes/intel_ndns/metric_fsb/exp/baseline_onlyGen_freq_MAE_mag_MAE/checkpoints/best/pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = BnbQuantizationConfig(load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_model = load_and_quantize_model(model, weights_location=model_weights_path, bnb_quantization_config=quantization_config, device_map = \"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Separator(\n",
       "  (fb_model): SequenceModel(\n",
       "    (sequence_model): LSTM(256, 512, num_layers=2)\n",
       "    (fc_output_layer): Linear8bitLt(in_features=512, out_features=256, bias=True)\n",
       "  )\n",
       "  (sb_model): SubbandModel(\n",
       "    (sb_models): ModuleList(\n",
       "      (0): SubBandSequenceWrapper(\n",
       "        (sequence_model): LSTM(32, 384, num_layers=2)\n",
       "        (fc_output_layer): Linear(in_features=384, out_features=2, bias=True)\n",
       "      )\n",
       "      (1): SubBandSequenceWrapper(\n",
       "        (sequence_model): LSTM(38, 384, num_layers=2)\n",
       "        (fc_output_layer): Linear(in_features=384, out_features=8, bias=True)\n",
       "      )\n",
       "      (2): SubBandSequenceWrapper(\n",
       "        (sequence_model): LSTM(46, 384, num_layers=2)\n",
       "        (fc_output_layer): Linear(in_features=384, out_features=16, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 3.6804e-02, -1.1353e-02, -1.3992e-02,  ..., -3.6157e-01,\n",
      "         -3.6572e-01, -4.0112e-01],\n",
      "        [-1.3281e-01,  1.7365e-02,  4.6326e-02,  ..., -2.7759e-01,\n",
      "         -3.1323e-01, -3.7305e-01],\n",
      "        [ 2.2471e-04, -2.0714e-03, -3.3112e-02,  ..., -5.4932e-02,\n",
      "         -2.7664e-02, -1.2871e-02],\n",
      "        ...,\n",
      "        [ 6.5269e-03,  1.6434e-02,  8.4229e-02,  ...,  1.2634e-01,\n",
      "          8.7463e-02,  7.8491e-02],\n",
      "        [-4.0283e-03,  3.7292e-02, -1.3634e-02,  ..., -1.0244e+00,\n",
      "         -9.3506e-01, -9.0820e-01],\n",
      "        [ 5.3375e-02, -8.5999e-02, -7.8979e-02,  ..., -2.2119e-01,\n",
      "         -1.3611e-01, -1.5442e-01]], device='cuda:1', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1084, -0.1876,  0.0748,  ..., -0.0488, -0.1032, -0.0525],\n",
      "        [-0.2087,  0.3213,  0.0557,  ..., -0.3269,  0.0764,  0.0063],\n",
      "        [-0.0291, -0.0123, -0.0052,  ..., -0.1144,  0.1355,  0.0394],\n",
      "        ...,\n",
      "        [ 0.1426, -0.0806,  0.0052,  ..., -0.1173, -0.0792,  0.0213],\n",
      "        [-0.0287, -0.1820, -0.0175,  ..., -0.0463,  0.1915,  0.0275],\n",
      "        [-0.0930,  0.1667,  0.0156,  ..., -0.2156,  0.0662, -0.2817]],\n",
      "       device='cuda:1', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0031, -0.0839, -0.1151,  ..., -0.0214,  0.0786, -0.2306],\n",
      "       device='cuda:1', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0161, -0.1199, -0.1082,  ..., -0.0250,  0.1197, -0.2151],\n",
      "       device='cuda:1', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0073, -0.0342,  0.0095,  ..., -0.0078, -0.0112, -0.1116],\n",
      "        [-0.1061,  0.1322,  0.0855,  ...,  0.1322,  0.0486, -0.0412],\n",
      "        [-0.1962, -0.0349,  0.0838,  ..., -0.0620,  0.1761, -0.0093],\n",
      "        ...,\n",
      "        [-0.1628,  0.0071, -0.0508,  ..., -0.0765, -0.0521, -0.0175],\n",
      "        [ 0.2434, -0.0657,  0.0494,  ...,  0.0568,  0.1986,  0.0598],\n",
      "        [ 0.1405,  0.1226, -0.0366,  ...,  0.1417, -0.1422,  0.0633]],\n",
      "       device='cuda:1', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1218, -0.0361,  0.0031,  ..., -0.2236,  0.0667,  0.0688],\n",
      "        [-0.0616, -0.0688, -0.0616,  ..., -0.0018,  0.0723,  0.1484],\n",
      "        [-0.0914,  0.0310, -0.1935,  ..., -0.0709, -0.0910,  0.2106],\n",
      "        ...,\n",
      "        [ 0.0675,  0.1334,  0.0637,  ..., -0.0410,  0.1123, -0.0302],\n",
      "        [ 0.0891, -0.0614,  0.1606,  ..., -0.0493,  0.0212, -0.0562],\n",
      "        [ 0.0570,  0.0635, -0.1586,  ...,  0.1100,  0.0695, -0.0301]],\n",
      "       device='cuda:1', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0524,  0.0057, -0.0689,  ...,  0.0807,  0.0194,  0.0343],\n",
      "       device='cuda:1', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0330, -0.0159, -0.0429,  ...,  0.0934,  0.0144, -0.0125],\n",
      "       device='cuda:1', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "Parameter(Int8Params([[ 31,  49,  -3,  ...,   4, -64,  52],\n",
      "            [ 17,  28,   1,  ...,  15, -50,  17],\n",
      "            [ 12,   6,  -2,  ...,   1, -30,   5],\n",
      "            ...,\n",
      "            [ -2,  -4, -19,  ..., -91,  47, -44],\n",
      "            [  6,  10, -19,  ..., -80,  16, -60],\n",
      "            [ -6,  -3, -16,  ..., -90,  24, -53]], device='cuda:2',\n",
      "           dtype=torch.int8))\n",
      "Parameter containing:\n",
      "tensor([ 9.8572e-02, -3.3081e-02, -7.2815e-02, -4.2877e-02, -5.7068e-02,\n",
      "        -2.7222e-02,  8.8692e-04, -1.1856e-02,  2.3575e-02, -7.7171e-03,\n",
      "        -6.3705e-03, -1.0063e-02, -1.0757e-02, -1.9119e-02, -6.3965e-02,\n",
      "        -7.7332e-02, -9.8145e-02, -1.0413e-01, -1.2372e-01, -1.1682e-01,\n",
      "         1.8143e-02, -9.0210e-02,  3.8757e-02, -1.0132e-01,  4.3396e-02,\n",
      "        -7.7087e-02,  4.7546e-02, -7.9224e-02,  1.1249e-01, -2.3972e-02,\n",
      "         5.2094e-02, -8.4900e-02,  8.4717e-02, -2.0615e-02,  3.4180e-02,\n",
      "        -1.0773e-01,  1.2976e-01, -2.5009e-02,  3.3054e-03, -8.8501e-02,\n",
      "         1.4783e-01,  3.0502e-02,  1.2497e-02, -1.1768e-01,  1.1725e-01,\n",
      "        -4.2267e-03, -4.3297e-04, -1.1267e-01,  1.3403e-01,  2.4170e-02,\n",
      "        -4.9362e-03, -7.8125e-02,  1.5833e-01,  4.4403e-02,  4.3488e-03,\n",
      "        -7.9285e-02,  1.5332e-01,  6.7017e-02, -3.1952e-02, -5.8014e-02,\n",
      "         1.3696e-01,  8.8989e-02,  1.8597e-03, -2.3758e-02,  1.3611e-01,\n",
      "         8.1482e-02, -2.3895e-02, -2.3254e-02,  1.0187e-01,  6.3660e-02,\n",
      "        -6.5613e-03, -4.1504e-02,  1.3171e-01,  9.0942e-02, -2.3880e-03,\n",
      "        -3.0167e-02,  1.3782e-01,  9.9670e-02, -2.2125e-02, -4.3823e-02,\n",
      "         6.2675e-03, -7.6904e-02, -1.5503e-01, -1.2367e-02, -4.8187e-02,\n",
      "        -4.1962e-02,  2.1936e-01, -1.9238e-01, -2.6413e-02, -4.0314e-02,\n",
      "        -1.0779e-01, -5.8594e-03, -4.3304e-02, -5.8594e-02,  1.9775e-01,\n",
      "        -1.6785e-01, -2.1027e-02, -5.3497e-02, -8.9966e-02, -1.0818e-02,\n",
      "        -2.4307e-02, -3.6743e-02,  1.9263e-01, -1.2305e-01,  9.7942e-04,\n",
      "        -2.6367e-02, -6.5186e-02, -3.5801e-03, -3.4515e-02, -6.8848e-02,\n",
      "         2.0923e-01, -1.2311e-01,  5.7068e-03, -5.3467e-02, -3.9276e-02,\n",
      "         3.6163e-02,  1.2962e-02, -5.3101e-02,  1.9641e-01, -1.6052e-01,\n",
      "         4.5280e-03, -6.6040e-02, -1.5266e-02,  4.4525e-02, -2.3590e-02,\n",
      "        -4.8096e-02,  2.1057e-01, -1.0944e-01,  1.9806e-02, -2.5070e-02,\n",
      "         8.1940e-03,  7.8003e-02, -1.6846e-02, -2.2385e-02,  2.5269e-01,\n",
      "        -5.7709e-02,  2.1042e-02, -2.0630e-02,  1.5419e-02,  1.0034e-01,\n",
      "        -2.6703e-02, -2.9541e-02,  2.5610e-01, -6.9702e-02, -2.0416e-02,\n",
      "        -1.7166e-02,  5.4443e-02,  5.8655e-02,  4.2786e-02, -2.0294e-02,\n",
      "         2.9077e-01, -3.6224e-02,  1.2665e-02, -3.1433e-02,  6.8970e-02,\n",
      "         7.9529e-02,  2.9953e-02, -3.5980e-02,  2.8125e-01, -8.7357e-03,\n",
      "        -1.4954e-02,  5.4054e-03,  6.4148e-02,  1.0016e-01,  9.0103e-03,\n",
      "        -3.8940e-02,  2.8149e-01, -4.5563e-02,  2.6352e-02, -2.3865e-02,\n",
      "         8.9355e-02,  1.1414e-01,  1.5778e-02, -3.9978e-02,  2.7588e-01,\n",
      "        -3.4393e-02,  2.2842e-02, -1.4286e-03,  8.7097e-02,  9.6130e-02,\n",
      "         2.9938e-02, -5.2216e-02,  3.1396e-01, -9.3307e-03,  1.1307e-02,\n",
      "        -2.6230e-02,  7.0435e-02,  9.7595e-02,  2.7710e-02, -5.7739e-02,\n",
      "         3.0884e-01,  1.0315e-02, -6.2218e-03, -2.3331e-02,  8.9172e-02,\n",
      "         8.7463e-02,  9.3155e-03, -7.3303e-02,  3.3838e-01,  1.5053e-02,\n",
      "         2.8366e-02,  5.0659e-03,  6.7688e-02,  5.6854e-02,  1.8646e-02,\n",
      "        -8.1421e-02,  3.0273e-01, -1.5396e-02,  2.8839e-02, -2.2919e-02,\n",
      "         8.5938e-02,  6.9397e-02,  2.2842e-02, -5.8624e-02,  3.5571e-01,\n",
      "         1.3412e-02,  4.1046e-02,  8.0795e-03,  9.0942e-02,  6.7322e-02,\n",
      "         5.6572e-03, -4.4800e-02,  3.5254e-01,  1.2024e-02,  6.7017e-02,\n",
      "        -2.3636e-02,  9.3079e-02,  8.6670e-02,  1.6663e-02, -4.0955e-02,\n",
      "         3.6621e-01, -6.5384e-03,  4.7333e-02,  1.0963e-02,  1.9934e-01,\n",
      "         1.3135e-01,  7.3975e-02, -2.4612e-02,  3.6182e-01, -5.7739e-02,\n",
      "         8.7280e-02,  5.1208e-02,  1.7419e-01,  1.4294e-01,  8.6548e-02,\n",
      "         7.7698e-02,  4.1382e-01, -6.9153e-02,  3.3276e-01,  3.2568e-01,\n",
      "         4.3555e-01,  4.2871e-01,  4.3140e-01,  3.2568e-01,  5.8203e-01,\n",
      "         3.8135e-01], device='cuda:2', dtype=torch.float16)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0071, -0.0062,  0.0894,  ..., -0.0105, -0.0328,  0.0467],\n",
      "        [-0.0040, -0.0294,  0.0402,  ..., -0.2070, -0.1016, -0.0849],\n",
      "        [-0.0112,  0.0363,  0.0986,  ..., -0.0852, -0.0901, -0.0223],\n",
      "        ...,\n",
      "        [-0.0240,  0.0471, -0.0106,  ..., -0.0049, -0.0674,  0.0923],\n",
      "        [ 0.0717,  0.0756, -0.0607,  ..., -0.0056, -0.0654, -0.0386],\n",
      "        [ 0.0736, -0.0213, -0.0620,  ...,  0.0255, -0.0634, -0.1565]],\n",
      "       device='cuda:2', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0774,  0.0219,  0.3477,  ...,  0.1431,  0.2103, -0.0801],\n",
      "        [-0.1268, -0.1266, -0.0227,  ..., -0.0317, -0.1449,  0.0573],\n",
      "        [ 0.1372, -0.1937, -0.0647,  ...,  0.0075, -0.0822, -0.1719],\n",
      "        ...,\n",
      "        [ 0.0688, -0.0696, -0.0251,  ..., -0.0133, -0.0424,  0.0119],\n",
      "        [ 0.0221, -0.1343, -0.0088,  ..., -0.0176,  0.0603, -0.0187],\n",
      "        [ 0.1509,  0.0743, -0.0687,  ...,  0.0488,  0.0410, -0.2290]],\n",
      "       device='cuda:2', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0930,  0.1199,  0.0632,  ..., -0.0535,  0.0036,  0.0257],\n",
      "       device='cuda:2', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1326,  0.0552,  0.0306,  ..., -0.0919,  0.0234,  0.0420],\n",
      "       device='cuda:2', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0586, -0.0018,  0.0757,  ..., -0.0015,  0.0663,  0.0755],\n",
      "        [-0.0049,  0.0213, -0.0649,  ..., -0.0149,  0.0065,  0.0382],\n",
      "        [-0.0264,  0.0413, -0.0439,  ..., -0.0750,  0.0837,  0.1147],\n",
      "        ...,\n",
      "        [ 0.2336, -0.0111, -0.0332,  ..., -0.0407,  0.0021,  0.0261],\n",
      "        [-0.0676,  0.0360,  0.0105,  ..., -0.0401,  0.0365,  0.0580],\n",
      "        [ 0.0909,  0.0961,  0.1272,  ..., -0.0557,  0.0116,  0.1259]],\n",
      "       device='cuda:2', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0392, -0.0947,  0.0599,  ..., -0.0871,  0.1904,  0.1189],\n",
      "        [-0.0333, -0.0201, -0.0660,  ..., -0.1790,  0.0767, -0.1752],\n",
      "        [ 0.0724, -0.0801,  0.0794,  ..., -0.0594,  0.0351, -0.1316],\n",
      "        ...,\n",
      "        [-0.1660, -0.0440, -0.0817,  ..., -0.0277,  0.2153, -0.0400],\n",
      "        [-0.0659, -0.0306, -0.0393,  ..., -0.0787, -0.1015, -0.0195],\n",
      "        [-0.1306,  0.0477, -0.0305,  ..., -0.0399,  0.0673,  0.0671]],\n",
      "       device='cuda:2', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0123,  0.0064,  0.0156,  ..., -0.0502, -0.0316,  0.0353],\n",
      "       device='cuda:2', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0018, -0.0379, -0.0392,  ..., -0.0125, -0.0248, -0.0134],\n",
      "       device='cuda:2', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.0132e-02,  3.4180e-02, -2.9297e-02, -1.1615e-01,  1.2695e-01,\n",
      "          6.0387e-03,  5.0568e-02,  2.3102e-02,  3.8330e-02,  3.5339e-02,\n",
      "         -2.3010e-02, -1.1501e-03, -4.0924e-02, -5.0934e-02,  5.5237e-02,\n",
      "         -4.0796e-01,  1.1591e-01, -6.5247e-02,  1.5610e-02,  3.0838e-02,\n",
      "          2.2629e-02,  3.6621e-02, -5.7098e-02,  1.8263e-03,  3.7781e-02,\n",
      "          1.4885e-02, -3.8948e-03, -2.0844e-02,  1.2598e-01,  9.1629e-03,\n",
      "          4.3549e-02,  4.7455e-02,  2.2106e-03, -4.1809e-02,  1.4744e-03,\n",
      "         -5.2783e-01, -1.6800e-02, -5.9013e-03, -2.3956e-02,  1.7105e-02,\n",
      "         -1.0785e-01,  1.5906e-01,  4.3304e-02,  5.2734e-02, -1.1353e-02,\n",
      "          7.8430e-02,  3.4428e-03,  1.1963e-01,  4.4250e-02,  3.4882e-02,\n",
      "         -1.9455e-03,  4.0436e-02,  2.3117e-02, -1.6223e-01,  1.0754e-01,\n",
      "          7.3792e-02, -6.6101e-02, -1.0651e-02, -2.8366e-02, -1.1345e-02,\n",
      "         -1.9287e-02,  1.4351e-02, -2.0161e-03, -4.0070e-02,  1.3260e-02,\n",
      "          6.5193e-03,  1.2115e-02, -1.4978e-01,  1.3153e-02,  1.0785e-01,\n",
      "          4.3304e-02, -3.0533e-02, -2.3849e-02, -2.9190e-02,  6.3324e-04,\n",
      "          2.6215e-02, -2.4445e-02,  1.5068e-02, -9.7778e-02, -9.6924e-02,\n",
      "         -1.9424e-02, -3.8818e-02, -1.1292e-02, -2.1317e-02,  5.2490e-03,\n",
      "          2.8275e-02, -5.5176e-02,  4.7699e-02,  5.0262e-02, -3.4033e-01,\n",
      "          3.0884e-01,  3.5095e-02,  2.4857e-02, -2.2141e-02,  1.4801e-02,\n",
      "         -3.5645e-02,  3.5645e-02,  6.8893e-03, -3.9001e-02, -6.0028e-02,\n",
      "          3.5229e-03,  4.8180e-03, -4.4373e-02,  6.2317e-02,  3.2177e-03,\n",
      "          1.6846e-02, -5.9128e-03,  5.4092e-03, -1.4091e-02,  3.0441e-02,\n",
      "         -2.2507e-02, -6.4160e-01, -5.2948e-02,  1.8402e-02, -1.1795e-02,\n",
      "         -2.5635e-03,  3.1311e-02, -4.4098e-02, -2.0264e-02,  1.3647e-03,\n",
      "         -1.3464e-01, -8.7463e-02,  6.7261e-02,  2.2110e-02,  4.6906e-02,\n",
      "         -2.0943e-03,  3.6133e-02, -2.9892e-02,  2.4433e-03, -3.8395e-03,\n",
      "          9.9335e-03,  1.6006e-02,  3.8971e-02,  7.1631e-01,  1.1864e-02,\n",
      "          4.0680e-02,  4.0222e-02, -4.2114e-02, -5.3711e-02, -6.1035e-03,\n",
      "         -2.3880e-03,  7.1983e-03,  5.0049e-02, -3.6938e-01, -5.4810e-02,\n",
      "         -1.7014e-02, -4.0070e-02, -2.4124e-02, -6.6711e-02, -1.2427e-01,\n",
      "         -5.6381e-03, -4.8637e-03, -2.8503e-02,  4.0924e-02,  8.9844e-02,\n",
      "          1.2292e-01,  4.2023e-02,  5.7007e-02, -5.0751e-02,  6.2132e-04,\n",
      "         -5.5206e-02, -4.0985e-02, -2.2163e-03, -6.3354e-02, -4.2822e-01,\n",
      "         -2.1095e-03, -1.0095e-01, -2.4395e-03, -2.9831e-03,  3.3447e-02,\n",
      "         -2.2522e-02, -1.3586e-01, -7.9956e-03,  3.2425e-05,  9.4971e-02,\n",
      "         -3.1708e-02,  4.5729e-04, -6.5857e-02, -8.5297e-03,  1.7130e-04,\n",
      "          1.7303e-02,  4.0833e-02, -1.4259e-02, -9.6893e-03,  8.7891e-03,\n",
      "         -2.9774e-03, -3.7050e-04, -1.2054e-03,  5.6580e-02, -9.1839e-04,\n",
      "         -5.9540e-02, -1.5854e-02, -1.2703e-02,  8.7280e-03,  2.8732e-02,\n",
      "          9.5215e-03, -1.7563e-02, -3.3173e-02,  4.9805e-02,  1.1692e-03,\n",
      "          7.4341e-02,  8.1863e-03,  8.7509e-03, -4.7836e-03, -1.2329e-02,\n",
      "          1.3802e-02,  3.5767e-02, -4.0741e-02,  3.8300e-02,  2.6596e-02,\n",
      "         -4.2480e-02,  9.1362e-04, -5.8479e-03,  2.7557e-02,  4.3884e-02,\n",
      "          3.2104e-02, -3.0655e-02,  7.6660e-02,  2.8976e-02, -5.7666e-01,\n",
      "          5.6641e-02, -1.1493e-01, -6.8426e-04,  4.9973e-03,  1.4050e-01,\n",
      "         -3.0632e-03, -4.5868e-02,  3.5156e-02, -4.9469e-02, -3.3997e-02,\n",
      "         -7.4730e-03, -2.9724e-02, -3.8635e-02, -4.6082e-02, -3.9001e-02,\n",
      "          4.3427e-02, -1.2032e-02, -1.6281e-02,  2.0654e-01, -6.6345e-02,\n",
      "          2.8336e-02, -4.0619e-02, -2.6657e-02,  4.6967e-02, -3.6102e-02,\n",
      "          3.0838e-02, -8.4991e-03,  2.0538e-02,  5.2185e-02, -6.1846e-04,\n",
      "          5.1117e-02, -1.3351e-02, -1.7426e-02,  2.5085e-02,  2.8961e-02,\n",
      "          1.2085e-02, -3.3936e-02, -2.1957e-02,  4.2664e-02, -3.6133e-02,\n",
      "         -3.0655e-02,  2.2781e-02,  6.2347e-02,  2.0966e-02, -4.7821e-02,\n",
      "         -9.5367e-03,  2.4506e-02,  5.7953e-02,  4.1687e-02, -7.0190e-02,\n",
      "         -4.5135e-02,  2.0386e-02, -5.1575e-02,  1.8173e-02,  6.5002e-03,\n",
      "         -1.0620e-02,  1.8967e-02,  4.2877e-02, -2.9205e-02, -3.2463e-03,\n",
      "          1.9569e-03, -4.9162e-04,  1.7468e-01, -3.2440e-02, -1.8539e-02,\n",
      "          3.1708e-02, -4.2725e-02, -3.7018e-02,  2.3483e-02,  5.0049e-02,\n",
      "          7.0374e-02,  9.2392e-03,  1.5381e-01,  3.8696e-02, -2.6108e-02,\n",
      "         -4.2152e-03,  1.9226e-02, -1.6983e-02,  3.9825e-02, -6.5735e-02,\n",
      "          8.3256e-04, -4.9072e-02,  2.1172e-03,  2.5511e-05, -4.9194e-02,\n",
      "         -3.8300e-02,  4.4525e-02, -2.1643e-01, -1.2646e-01,  1.9836e-02,\n",
      "         -4.4922e-02,  1.1517e-01,  2.0544e-01,  6.3293e-02,  4.8279e-02,\n",
      "         -1.3269e-01, -2.6550e-03, -6.2378e-02, -1.0178e-02,  1.5961e-02,\n",
      "          1.4099e-01,  1.1818e-02, -2.6108e-02, -6.5063e-02, -1.8127e-02,\n",
      "         -3.2074e-02,  1.0300e-02,  7.3051e-03, -4.6783e-02, -3.0167e-02,\n",
      "          1.4258e-01, -2.2278e-02,  2.5604e-02, -1.7624e-02, -1.3931e-02,\n",
      "         -3.9215e-02, -7.3395e-03, -3.5095e-02, -3.0575e-03,  4.4022e-03,\n",
      "         -3.6669e-04,  9.1431e-02,  2.1774e-02, -7.2656e-01, -6.8970e-02,\n",
      "          9.7778e-02,  2.3010e-01, -2.3003e-03,  1.1346e-01,  1.5793e-02,\n",
      "          4.9515e-03, -1.1139e-02, -7.5256e-02,  2.1759e-02,  2.2980e-02,\n",
      "         -3.2806e-02, -5.6689e-01, -4.3610e-02, -7.8247e-02,  6.0272e-02,\n",
      "         -4.2603e-02, -9.7580e-03,  2.4109e-02, -5.3833e-02,  7.7637e-02,\n",
      "         -5.2917e-02,  2.2049e-03,  3.7048e-02,  2.8641e-02, -1.3684e-01,\n",
      "          2.4994e-02,  1.0284e-01, -9.9850e-04,  3.1128e-03,  2.5375e-02,\n",
      "         -1.4778e-02,  2.7710e-02, -3.3844e-02,  1.6357e-02, -1.1154e-02,\n",
      "         -1.9806e-02,  5.3009e-02, -7.7782e-03,  8.3160e-03],\n",
      "        [ 1.1383e-02,  3.3386e-02, -3.4058e-02, -1.1133e-01,  1.2830e-01,\n",
      "          7.4387e-03,  2.1271e-02,  2.6489e-02,  3.7598e-02,  3.0014e-02,\n",
      "         -3.0991e-02, -1.9178e-03, -3.9886e-02, -3.7323e-02,  5.3802e-02,\n",
      "         -4.1528e-01,  1.2170e-01, -6.7749e-02,  9.8877e-03,  2.5787e-02,\n",
      "          2.1835e-02,  3.6560e-02, -5.6732e-02,  2.8648e-03,  3.5065e-02,\n",
      "          1.5541e-02, -3.3760e-03, -2.7252e-02,  1.2134e-01,  3.8544e-02,\n",
      "          3.8788e-02,  4.3701e-02,  2.9716e-03, -2.8229e-02,  3.1738e-03,\n",
      "         -5.2539e-01, -4.6082e-02, -4.7417e-03, -2.3209e-02,  1.6357e-02,\n",
      "         -1.0651e-01,  1.5735e-01,  4.3671e-02,  5.5359e-02, -1.0468e-02,\n",
      "          7.9651e-02,  4.6425e-03,  1.2164e-01,  4.0710e-02,  3.9429e-02,\n",
      "         -2.6531e-03,  2.9648e-02,  2.1973e-02, -1.6431e-01,  9.7046e-02,\n",
      "          7.1594e-02, -6.9031e-02, -1.5930e-02, -2.1576e-02, -1.2062e-02,\n",
      "         -1.8036e-02,  1.1360e-02, -2.8458e-03, -4.2236e-02,  1.4984e-02,\n",
      "          9.0866e-03,  1.0979e-02, -1.4392e-01,  2.2293e-02,  1.0620e-01,\n",
      "          4.2542e-02, -3.8879e-02, -2.5848e-02, -2.9236e-02, -3.6001e-05,\n",
      "          9.3445e-02, -1.7120e-02,  1.1230e-02, -9.2163e-02, -9.2224e-02,\n",
      "         -1.3916e-02, -4.3365e-02, -2.6398e-02, -1.6586e-02,  4.3983e-03,\n",
      "          2.8564e-02, -2.9938e-02,  5.1575e-02,  4.4556e-02, -3.3789e-01,\n",
      "          3.0322e-01,  4.0680e-02,  2.4002e-02, -3.1647e-02,  1.0612e-02,\n",
      "         -3.0075e-02,  2.7496e-02,  8.5831e-03,  3.4485e-03, -5.9326e-02,\n",
      "          5.1956e-03,  6.3362e-03, -4.1962e-02,  4.6722e-02,  4.1428e-03,\n",
      "          2.6596e-02, -4.5319e-03,  4.8790e-03, -5.2376e-03,  2.9587e-02,\n",
      "         -2.2278e-02, -6.4697e-01, -4.5013e-02,  1.7487e-02, -1.0689e-02,\n",
      "         -2.5673e-03,  2.8320e-02, -3.1738e-02, -2.1606e-02,  3.8242e-04,\n",
      "         -1.3672e-01, -7.8003e-02,  6.1829e-02,  2.0233e-02,  4.5898e-02,\n",
      "         -2.7283e-02,  2.3819e-02, -2.5406e-02,  5.7602e-03, -6.5575e-03,\n",
      "          1.9104e-02,  1.6159e-02,  3.7994e-02,  7.0459e-01,  8.8196e-03,\n",
      "          2.7527e-02,  3.9093e-02, -4.2328e-02, -6.1829e-02, -6.3667e-03,\n",
      "         -1.9102e-03,  6.9809e-03,  1.5404e-02, -3.7134e-01, -4.7119e-02,\n",
      "         -1.7731e-02, -4.0466e-02, -1.8509e-02, -6.8909e-02, -1.2219e-01,\n",
      "         -5.2490e-03, -3.5461e-02, -2.8671e-02,  3.8177e-02,  9.5215e-02,\n",
      "          1.4807e-01,  4.1595e-02,  5.3772e-02, -3.8910e-02,  1.1215e-03,\n",
      "         -4.4464e-02, -4.4800e-02, -8.7738e-04, -6.1890e-02, -4.3262e-01,\n",
      "         -1.0118e-03, -1.0443e-01, -4.0092e-03, -5.2032e-03,  5.7098e-02,\n",
      "         -2.2598e-02, -1.3525e-01, -8.0490e-03, -4.0855e-03,  1.0620e-01,\n",
      "         -3.3752e-02,  4.8828e-04, -6.0547e-02, -7.1869e-03,  6.4802e-04,\n",
      "          4.1321e-02,  2.3682e-02, -1.4236e-02, -1.2512e-02,  8.9722e-03,\n",
      "         -3.8090e-03,  1.2720e-04, -6.0558e-04,  5.5817e-02,  9.0742e-04,\n",
      "         -5.7739e-02, -2.4414e-02, -1.7838e-02,  1.1917e-02,  3.3722e-02,\n",
      "          9.3079e-03, -1.9196e-02, -1.8280e-02,  4.2450e-02,  6.9666e-04,\n",
      "          7.0557e-02,  7.1678e-03,  8.7814e-03, -5.3253e-03, -1.3611e-02,\n",
      "          1.9638e-02,  4.2053e-02, -1.2810e-02,  3.6438e-02,  2.8351e-02,\n",
      "         -4.3976e-02,  1.3123e-03, -5.6114e-03,  2.3132e-02,  3.6377e-02,\n",
      "          3.2166e-02, -2.8915e-02,  7.4097e-02,  1.3596e-02, -5.7910e-01,\n",
      "          5.2948e-02, -1.2085e-01, -7.1883e-05,  4.4823e-03,  1.4136e-01,\n",
      "         -4.5891e-03, -1.2772e-02,  4.0009e-02, -1.9058e-02, -3.1311e-02,\n",
      "         -8.5068e-03, -6.9580e-02, -3.1372e-02, -5.1392e-02, -4.0771e-02,\n",
      "          3.5645e-02, -1.0857e-02, -1.6617e-02,  1.9861e-01, -5.9235e-02,\n",
      "          1.8677e-02, -3.3356e-02, -4.3121e-02,  4.8065e-02, -1.7838e-02,\n",
      "          3.0380e-02, -3.2082e-03,  2.4506e-02, -3.9917e-01, -6.2275e-04,\n",
      "          3.5156e-02, -2.6062e-02, -2.2110e-02,  3.6438e-02,  1.9165e-02,\n",
      "          1.1559e-02, -3.9459e-02, -2.2446e-02,  5.2399e-02, -2.7802e-02,\n",
      "         -3.0228e-02,  2.4567e-02,  6.2622e-02,  2.0584e-02, -2.2629e-02,\n",
      "         -7.6065e-03,  1.5945e-02,  5.1178e-02,  6.4468e-03, -6.5308e-02,\n",
      "         -1.4626e-02,  1.9989e-02, -5.2094e-02,  1.9180e-02,  7.0724e-03,\n",
      "         -9.4147e-03,  2.7466e-02,  3.2898e-02, -1.2749e-02, -3.2310e-03,\n",
      "          2.4605e-03, -6.0320e-05,  1.7188e-01, -6.3293e-02, -2.2552e-02,\n",
      "          2.1839e-03, -3.3539e-02, -2.2903e-02,  2.6077e-02,  4.4647e-02,\n",
      "          7.3181e-02,  9.6436e-03,  1.6028e-01,  3.4424e-02, -5.0446e-02,\n",
      "         -2.8400e-03,  4.2236e-02, -1.6418e-02,  4.1687e-02, -6.8665e-02,\n",
      "         -2.2197e-04, -2.0142e-02,  4.2534e-03,  4.3392e-04, -5.6122e-02,\n",
      "         -1.8463e-02,  4.1962e-02, -2.1326e-01, -1.2292e-01,  2.3804e-02,\n",
      "         -4.5258e-02,  1.3025e-01,  2.0679e-01,  3.8452e-02,  5.3680e-02,\n",
      "         -1.2854e-01, -7.5226e-03, -4.0009e-02, -1.0506e-02,  1.9379e-02,\n",
      "          1.5027e-01,  1.3161e-02, -2.9877e-02, -6.5796e-02, -1.7639e-02,\n",
      "         -3.1433e-02,  9.1400e-03,  6.1340e-03, -3.9703e-02, -3.7903e-02,\n",
      "          1.4954e-01, -1.4503e-02,  3.4027e-02, -1.1833e-02, -1.4893e-02,\n",
      "         -3.5645e-02, -1.2535e-02, -3.7231e-02, -5.1193e-03,  3.0479e-03,\n",
      "         -7.0333e-04,  4.8401e-02,  4.4922e-02, -7.3242e-01, -7.7637e-02,\n",
      "          9.6802e-02,  2.2058e-01, -2.1210e-03,  1.1188e-01,  1.3100e-02,\n",
      "          2.4891e-03, -1.4542e-02, -7.2571e-02,  2.2293e-02,  1.9241e-02,\n",
      "         -1.7181e-02, -5.6396e-01, -2.1229e-03, -7.7576e-02,  4.1046e-02,\n",
      "         -5.7755e-03, -1.0597e-02,  2.3361e-02, -3.4912e-02,  7.1533e-02,\n",
      "         -5.5511e-02,  2.6588e-03,  3.7781e-02,  4.0497e-02, -1.2152e-01,\n",
      "          2.5757e-02,  9.7778e-02, -3.1967e-03,  3.5114e-03,  1.9760e-02,\n",
      "         -1.5701e-02,  2.4811e-02, -2.3300e-02,  1.7105e-02, -4.3976e-02,\n",
      "         -5.9692e-02,  5.1361e-02, -7.4158e-03,  7.9498e-03]], device='cuda:2',\n",
      "       dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0265, 0.0851], device='cuda:2', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.1357,  0.1912,  0.1166,  ..., -0.0713, -0.1477, -0.1295],\n",
      "        [ 0.0856,  0.0062,  0.0090,  ..., -0.0233, -0.0306,  0.0135],\n",
      "        [ 0.0477,  0.2000,  0.2117,  ..., -0.0304,  0.2712,  0.2502],\n",
      "        ...,\n",
      "        [ 0.0904,  0.0459,  0.0231,  ...,  0.0763,  0.0112, -0.0760],\n",
      "        [ 0.0028,  0.0481,  0.0321,  ...,  0.1108, -0.0532, -0.0258],\n",
      "        [ 0.0328,  0.0054, -0.0314,  ..., -0.2612,  0.3950,  0.0029]],\n",
      "       device='cuda:3', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0666,  0.0431, -0.0711,  ..., -0.0959,  0.1056, -0.0103],\n",
      "        [-0.1057,  0.1768,  0.1418,  ...,  0.0412,  0.0317, -0.3423],\n",
      "        [-0.0399, -0.3196,  0.4253,  ...,  0.2361,  0.0880,  0.0676],\n",
      "        ...,\n",
      "        [ 0.1895,  0.0393, -0.0434,  ...,  0.1609, -0.2520, -0.0639],\n",
      "        [ 0.0402,  0.1918,  0.0363,  ..., -0.2052, -0.0784, -0.0636],\n",
      "        [-0.0173, -0.0992,  0.1686,  ...,  0.1131, -0.0041, -0.3149]],\n",
      "       device='cuda:3', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1076, -0.0240,  0.1288,  ...,  0.0374, -0.0629,  0.0775],\n",
      "       device='cuda:3', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.1526, -0.0258,  0.1050,  ...,  0.0189,  0.0023,  0.0774],\n",
      "       device='cuda:3', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0828, -0.0494,  0.0687,  ...,  0.3215, -0.0168, -0.1920],\n",
      "        [-0.1871,  0.1831, -0.0586,  ...,  0.1780,  0.0251, -0.0746],\n",
      "        [ 0.0013, -0.0169,  0.1300,  ...,  0.0614,  0.0498,  0.2761],\n",
      "        ...,\n",
      "        [ 0.0063, -0.0948,  0.0033,  ..., -0.0196, -0.0193,  0.0016],\n",
      "        [-0.0070,  0.1628, -0.0690,  ..., -0.0558,  0.0494, -0.1118],\n",
      "        [-0.0005,  0.0443, -0.0210,  ...,  0.0305, -0.0039, -0.1212]],\n",
      "       device='cuda:3', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0945,  0.1449, -0.0623,  ..., -0.0091, -0.0950, -0.0299],\n",
      "        [ 0.0366, -0.1056, -0.0586,  ..., -0.0548, -0.0638,  0.1492],\n",
      "        [ 0.0015,  0.1364,  0.1735,  ...,  0.0114,  0.0353,  0.0201],\n",
      "        ...,\n",
      "        [-0.1088, -0.0504,  0.0842,  ..., -0.0422, -0.1212, -0.0760],\n",
      "        [-0.0642, -0.1603,  0.0376,  ...,  0.0380,  0.0834,  0.0681],\n",
      "        [ 0.0254,  0.0671, -0.0515,  ...,  0.0417, -0.2979,  0.1395]],\n",
      "       device='cuda:3', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1146, -0.1373, -0.0212,  ..., -0.0006,  0.0176, -0.0389],\n",
      "       device='cuda:3', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0471, -0.1400, -0.0276,  ..., -0.0159,  0.0173, -0.0644],\n",
      "       device='cuda:3', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0453, -0.0006,  0.0403,  ..., -0.0446, -0.0041, -0.1018],\n",
      "        [ 0.0609,  0.0036, -0.0437,  ...,  0.1129,  0.0214,  0.0184],\n",
      "        [ 0.0070,  0.0033,  0.1121,  ..., -0.0130, -0.0241,  0.0696],\n",
      "        ...,\n",
      "        [ 0.0382, -0.0035, -0.0446,  ...,  0.1039,  0.0224,  0.0146],\n",
      "        [ 0.0406,  0.0041,  0.1113,  ..., -0.0078, -0.0231,  0.0721],\n",
      "        [ 0.0514, -0.0019, -0.3213,  ...,  0.0690,  0.0312, -0.0988]],\n",
      "       device='cuda:3', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.0222, 0.0365, 0.0583, 0.0002, 0.0275, 0.0358, 0.0583, 0.0305],\n",
      "       device='cuda:3', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0349, -0.0137, -0.0158,  ...,  0.0447,  0.0151, -0.0549],\n",
      "        [ 0.0443, -0.0113,  0.0237,  ...,  0.0376,  0.1749, -0.0593],\n",
      "        [ 0.0326,  0.0251,  0.0365,  ..., -0.0065,  0.0660, -0.0836],\n",
      "        ...,\n",
      "        [-0.0850, -0.0273, -0.0163,  ...,  0.0497,  0.0300,  0.0798],\n",
      "        [ 0.0191,  0.0436,  0.0015,  ...,  0.1865, -0.1899,  0.0218],\n",
      "        [-0.2134, -0.1355, -0.1984,  ..., -0.1078, -0.0067,  0.0083]],\n",
      "       device='cuda:4', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0655,  0.0057,  0.1221,  ..., -0.0368, -0.1177,  0.0246],\n",
      "        [-0.1542, -0.0997, -0.0071,  ..., -0.0239,  0.1046, -0.0590],\n",
      "        [-0.1957, -0.1656,  0.1165,  ..., -0.0367,  0.0733, -0.0320],\n",
      "        ...,\n",
      "        [ 0.0890, -0.1564, -0.1093,  ...,  0.1548, -0.0429,  0.1144],\n",
      "        [ 0.1469,  0.1814,  0.1044,  ..., -0.0667, -0.0771,  0.0644],\n",
      "        [ 0.0187,  0.0511,  0.0591,  ..., -0.0903, -0.0651, -0.0021]],\n",
      "       device='cuda:4', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0551, -0.2080, -0.2032,  ...,  0.1873,  0.1124,  0.1172],\n",
      "       device='cuda:4', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0297, -0.2223, -0.1951,  ...,  0.1851,  0.0646,  0.1388],\n",
      "       device='cuda:4', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0677,  0.0870,  0.0266,  ...,  0.0149, -0.1204, -0.1682],\n",
      "        [-0.1163, -0.0464,  0.0560,  ..., -0.3345,  0.2563, -0.2094],\n",
      "        [ 0.0806,  0.0184, -0.0242,  ..., -0.1060,  0.1891, -0.0160],\n",
      "        ...,\n",
      "        [ 0.0485, -0.1059,  0.0420,  ..., -0.1385,  0.1046, -0.0230],\n",
      "        [-0.1630, -0.0385,  0.0038,  ..., -0.2571,  0.1029, -0.2244],\n",
      "        [ 0.0559, -0.0452, -0.0221,  ..., -0.0714, -0.0807, -0.0790]],\n",
      "       device='cuda:4', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 1.2256e-01,  3.5498e-01, -2.0813e-02,  ..., -3.4760e-02,\n",
      "          1.2329e-01, -5.2368e-02],\n",
      "        [-1.6858e-01, -4.9683e-02, -1.6006e-02,  ...,  1.0962e-01,\n",
      "         -5.8350e-01, -1.6980e-01],\n",
      "        [ 4.0131e-03, -1.4392e-01, -2.0374e-01,  ...,  1.1523e-01,\n",
      "          2.0630e-02, -2.6343e-01],\n",
      "        ...,\n",
      "        [-4.9164e-02, -7.7454e-02, -5.7793e-03,  ...,  4.7559e-01,\n",
      "          4.6167e-01, -9.9487e-02],\n",
      "        [-1.6708e-02,  1.6861e-02,  1.2793e-01,  ..., -4.3091e-02,\n",
      "          1.2329e-01,  1.5332e-01],\n",
      "        [-1.7566e-01,  1.1853e-01,  1.3672e-01,  ...,  6.3538e-02,\n",
      "          4.2542e-02,  2.4986e-04]], device='cuda:4', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0136, -0.1161, -0.0361,  ..., -0.0583, -0.1169,  0.0623],\n",
      "       device='cuda:4', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0047, -0.1445, -0.0032,  ..., -0.0641, -0.1716,  0.0229],\n",
      "       device='cuda:4', dtype=torch.float16, requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-1.6069e-03, -1.9690e-01, -8.4351e-02,  ...,  3.5126e-02,\n",
      "         -2.3315e-02, -2.4796e-04],\n",
      "        [ 2.0695e-03, -2.5220e-01,  7.3303e-02,  ...,  5.7343e-02,\n",
      "         -4.7241e-02, -7.3586e-03],\n",
      "        [-8.5831e-04,  2.2659e-02,  3.4973e-02,  ..., -1.5625e-01,\n",
      "         -2.0264e-01, -2.0081e-02],\n",
      "        ...,\n",
      "        [-1.2302e-03,  4.1046e-02,  9.7412e-02,  ...,  2.2507e-03,\n",
      "          2.5208e-02, -5.2071e-03],\n",
      "        [-6.5804e-04,  1.7685e-02,  1.2976e-01,  ...,  1.0971e-02,\n",
      "         -3.4351e-01,  9.3842e-03],\n",
      "        [-1.7052e-03, -7.1716e-02, -1.6699e-01,  ...,  3.6133e-02,\n",
      "          1.2219e-01,  2.9251e-02]], device='cuda:4', dtype=torch.float16,\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0119,  0.0547,  0.0355, -0.0055,  0.0434,  0.0104,  0.0231,  0.0113,\n",
      "         0.0230,  0.0327,  0.0411,  0.0126,  0.0406,  0.0255,  0.0142,  0.0032],\n",
      "       device='cuda:4', dtype=torch.float16, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for i in quantized_model.parameters():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not the same dtype, found input tensor with Float and parameter tensor with Half",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m quantized_model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      3\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m, \u001b[39m16000\u001b[39m, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mhalf)\n\u001b[0;32m----> 4\u001b[0m out \u001b[39m=\u001b[39m quantized_model(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m~/mambaforge/envs/audiozen_latest_accelerate/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/audiozen_latest_accelerate/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "Cell \u001b[0;32mIn[2], line 504\u001b[0m, in \u001b[0;36mSeparator.forward\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    502\u001b[0m noisy_mag \u001b[39m=\u001b[39m noisy_mag[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, :\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]  \u001b[39m# [B, 1, F, T]\u001b[39;00m\n\u001b[1;32m    503\u001b[0m fb_input \u001b[39m=\u001b[39m rearrange(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm(noisy_mag), \u001b[39m\"\u001b[39m\u001b[39mb c f t -> b (c f) t\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 504\u001b[0m fb_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfb_model(fb_input)  \u001b[39m# [B, F, T]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m fb_output \u001b[39m=\u001b[39m rearrange(fb_output, \u001b[39m\"\u001b[39m\u001b[39mb f t -> b 1 f t\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    507\u001b[0m \u001b[39m# ================== Subband ==================\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/audiozen_latest_accelerate/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[2], line 98\u001b[0m, in \u001b[0;36mSequenceModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     95\u001b[0m     states \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     97\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mpermute(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcontiguous()  \u001b[39m# [B, F, T] => [T, B, F]\u001b[39;00m\n\u001b[0;32m---> 98\u001b[0m o, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msequence_model(x, states)  \u001b[39m# [T, B, F] => [T, B, F]\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_size:\n\u001b[1;32m    101\u001b[0m     o \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_output_layer(o)  \u001b[39m# [T, B, F] => [T, B, F]\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/audiozen_latest_accelerate/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/mambaforge/envs/audiozen_latest_accelerate/lib/python3.10/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/mambaforge/envs/audiozen_latest_accelerate/lib/python3.10/site-packages/torch/nn/modules/rnn.py:812\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_forward_args(\u001b[39minput\u001b[39m, hx, batch_sizes)\n\u001b[1;32m    811\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 812\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39;49mlstm(\u001b[39minput\u001b[39;49m, hx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_flat_weights, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_layers,\n\u001b[1;32m    813\u001b[0m                       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdropout, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbidirectional, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_first)\n\u001b[1;32m    814\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    815\u001b[0m     result \u001b[39m=\u001b[39m _VF\u001b[39m.\u001b[39mlstm(\u001b[39minput\u001b[39m, batch_sizes, hx, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias,\n\u001b[1;32m    816\u001b[0m                       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input and parameter tensors are not the same dtype, found input tensor with Float and parameter tensor with Half"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    quantized_model.eval()\n",
    "    input = torch.rand(1, 16000, dtype=torch.half)\n",
    "    out = quantized_model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audiozen_latest_accelerate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
